{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks #\n",
    "\n",
    "In this notebook we are going to take a look neural networks and construct one using python 3\n",
    "\n",
    "Our goal is to make something that takes in some input and detects a pattern to produce an output.  The problem here is that while the human brain is exceptional at pattern detection this is no small task for a computer.\n",
    "\n",
    "This notebook is was built on a notebook from machine learning mastery: https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/ . We adapted it to out dataset but we used the Iris dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from random import random\n",
    "from csv import reader\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in and clean data ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we will be using in this notebook is the iris dataset.  It is a classic example that you should recognize from the lecture on decision trees.  It dates back to a paper from 1936 and contains one class that is linearly separable from the other two, while the other two are not linearly separable from each other.  This means that a single layer perceptron will NOT be able to categorize it properly, and we need a multi-layer perceptron.\n",
    "\n",
    "We need to first clean and normalize our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "#This is our dataset.  It is a classic example that you should recognise from the lecture on Decision Trees.\n",
    "#It dates back to a paper from 1936 and contains one class that is linearly separable from the other two,\n",
    "#while those two classes are not linearly separable from each other.\n",
    "irises = load_csv('bezdekIris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put columns on a right format\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6786c9d3d539>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mirises\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mstr_column_to_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mirises\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# convert class column to integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstr_column_to_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mirises\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mirises\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-ed5396272bfb>\u001b[0m in \u001b[0;36mstr_column_to_float\u001b[0;34m(dataset, column)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstr_column_to_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "for i in range(len(irises[0]) - 1):\n",
    "    str_column_to_float(irises, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(irises, len(irises[0]) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_split(dataset, folds_number):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / folds_number)\n",
    "    for i in range(folds_number):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    stats = [[min(column), max(column)] for column in zip(*dataset)]\n",
    "    return stats\n",
    "\n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)-1):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward and Back Propagate##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are used to feed training data to the network, and adjust weights using stochastic gradient descent.\n",
    "This version uses a sigmoid function as the transfer function as it is easy to derive, something that is necessary for gradient descent.\n",
    "\n",
    "Why the backpropagation does exactly what it does comes down to a bit of calculus.  You can find a derivation of the math behind backpropagation by Ryan Harris on youtube if you're curious, but the backbone of it is that weights are changed according to how much they affect the error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate the weights * inputs + bias\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation\n",
    "#this returns the sigmoid of x!\n",
    "def transfer(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "#The derivative of the sigmoid function\n",
    "def deriv(x):\n",
    "    return x*(1-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed forward ###\n",
    "Feeds inputs through the network and returns the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feed_forward(network, inputs):\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward propagation ###\n",
    "Propagate errors backwards through the network, one layer at time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_propagate(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        \n",
    "        # Derivative is diffrent for hidden layers and output layers\n",
    "        if i != len(network) - 1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * deriv(neuron['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting weights ###\n",
    "Update all the weights based on the propagated error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += l_rate * neuron['delta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where it all comes together.  This function feeds a single training input in, backpropagates the error and updates the weights accordingly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network(network, train, l_rate, n_epoch, expected_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            outputs = feed_forward(network, row)\n",
    "            expected = [0 for i in range(expected_outputs)]\n",
    "            expected[row[-1]] = 1\n",
    "            sum_error += sum([(expected[i] - outputs[i]) ** 2 for i in range(len(expected))])\n",
    "            backward_propagate(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "        if epoch % 20 == 0:\n",
    "            print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize and test out algorithm #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the functionality is implemented, we can see how the back propagation algorithm does.\n",
    "\n",
    "We initialize the network with one hidden layer as we need to learn to categorize non-linearly-separable data.  We are not limited to one layer, but for our purposes one hidden layer performs just fine.\n",
    "The weights we start with are small random numbers, and the last weight in the \"weights\" list represents the bias of the node that the weight list belongs to.\n",
    "\n",
    "To gain more intuition for the backpropagation algorithm, try experimenting with adding more nodes, hidden layers and changing the n_epoch (number of epochs: the number of times we iterate through the training data) and l_rate (learning rate) to see their impact on accuracy and speed.\n",
    "\n",
    "The code outputs current epoch number, learning rate and errors for every 20th epoch circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.300, error=100.072\n",
      ">epoch=20, lrate=0.300, error=84.545\n",
      ">epoch=40, lrate=0.300, error=44.385\n",
      ">epoch=60, lrate=0.300, error=40.170\n",
      ">epoch=80, lrate=0.300, error=19.624\n",
      ">epoch=100, lrate=0.300, error=16.094\n",
      ">epoch=120, lrate=0.300, error=15.406\n",
      ">epoch=140, lrate=0.300, error=14.901\n",
      ">epoch=160, lrate=0.300, error=13.317\n",
      ">epoch=180, lrate=0.300, error=8.370\n",
      ">epoch=200, lrate=0.300, error=7.308\n",
      ">epoch=220, lrate=0.300, error=6.712\n",
      ">epoch=240, lrate=0.300, error=14.484\n",
      ">epoch=260, lrate=0.300, error=7.518\n",
      ">epoch=280, lrate=0.300, error=8.001\n",
      ">epoch=300, lrate=0.300, error=6.512\n",
      ">epoch=320, lrate=0.300, error=6.411\n",
      ">epoch=340, lrate=0.300, error=2.649\n",
      ">epoch=360, lrate=0.300, error=12.856\n",
      ">epoch=380, lrate=0.300, error=4.122\n",
      ">epoch=400, lrate=0.300, error=5.786\n",
      ">epoch=420, lrate=0.300, error=2.320\n",
      ">epoch=440, lrate=0.300, error=2.389\n",
      ">epoch=460, lrate=0.300, error=3.153\n",
      ">epoch=480, lrate=0.300, error=7.198\n",
      ">epoch=0, lrate=0.300, error=110.900\n",
      ">epoch=20, lrate=0.300, error=44.443\n",
      ">epoch=40, lrate=0.300, error=42.464\n",
      ">epoch=60, lrate=0.300, error=41.443\n",
      ">epoch=80, lrate=0.300, error=40.676\n",
      ">epoch=100, lrate=0.300, error=39.858\n",
      ">epoch=120, lrate=0.300, error=39.101\n",
      ">epoch=140, lrate=0.300, error=39.413\n",
      ">epoch=160, lrate=0.300, error=39.308\n",
      ">epoch=180, lrate=0.300, error=38.692\n",
      ">epoch=200, lrate=0.300, error=38.587\n",
      ">epoch=220, lrate=0.300, error=38.646\n",
      ">epoch=240, lrate=0.300, error=38.140\n",
      ">epoch=260, lrate=0.300, error=38.055\n",
      ">epoch=280, lrate=0.300, error=37.531\n",
      ">epoch=300, lrate=0.300, error=37.499\n",
      ">epoch=320, lrate=0.300, error=37.181\n",
      ">epoch=340, lrate=0.300, error=37.821\n",
      ">epoch=360, lrate=0.300, error=37.593\n",
      ">epoch=380, lrate=0.300, error=37.214\n",
      ">epoch=400, lrate=0.300, error=37.828\n",
      ">epoch=420, lrate=0.300, error=36.352\n",
      ">epoch=440, lrate=0.300, error=37.286\n",
      ">epoch=460, lrate=0.300, error=36.583\n",
      ">epoch=480, lrate=0.300, error=36.964\n",
      ">epoch=0, lrate=0.300, error=123.447\n",
      ">epoch=20, lrate=0.300, error=84.710\n",
      ">epoch=40, lrate=0.300, error=42.250\n",
      ">epoch=60, lrate=0.300, error=22.317\n",
      ">epoch=80, lrate=0.300, error=16.851\n",
      ">epoch=100, lrate=0.300, error=11.084\n",
      ">epoch=120, lrate=0.300, error=11.643\n",
      ">epoch=140, lrate=0.300, error=12.375\n",
      ">epoch=160, lrate=0.300, error=11.482\n",
      ">epoch=180, lrate=0.300, error=12.239\n",
      ">epoch=200, lrate=0.300, error=16.723\n",
      ">epoch=220, lrate=0.300, error=9.598\n",
      ">epoch=240, lrate=0.300, error=12.448\n",
      ">epoch=260, lrate=0.300, error=13.011\n",
      ">epoch=280, lrate=0.300, error=15.589\n",
      ">epoch=300, lrate=0.300, error=12.742\n",
      ">epoch=320, lrate=0.300, error=10.455\n",
      ">epoch=340, lrate=0.300, error=11.624\n",
      ">epoch=360, lrate=0.300, error=11.419\n",
      ">epoch=380, lrate=0.300, error=8.796\n",
      ">epoch=400, lrate=0.300, error=10.422\n",
      ">epoch=420, lrate=0.300, error=9.652\n",
      ">epoch=440, lrate=0.300, error=10.988\n",
      ">epoch=460, lrate=0.300, error=9.902\n",
      ">epoch=480, lrate=0.300, error=11.062\n",
      ">epoch=0, lrate=0.300, error=105.267\n",
      ">epoch=20, lrate=0.300, error=41.058\n",
      ">epoch=40, lrate=0.300, error=18.831\n",
      ">epoch=60, lrate=0.300, error=10.672\n",
      ">epoch=80, lrate=0.300, error=11.273\n",
      ">epoch=100, lrate=0.300, error=7.944\n",
      ">epoch=120, lrate=0.300, error=9.730\n",
      ">epoch=140, lrate=0.300, error=17.029\n",
      ">epoch=160, lrate=0.300, error=10.682\n",
      ">epoch=180, lrate=0.300, error=9.791\n",
      ">epoch=200, lrate=0.300, error=9.465\n",
      ">epoch=220, lrate=0.300, error=8.184\n",
      ">epoch=240, lrate=0.300, error=9.690\n",
      ">epoch=260, lrate=0.300, error=7.856\n",
      ">epoch=280, lrate=0.300, error=8.583\n",
      ">epoch=300, lrate=0.300, error=6.465\n",
      ">epoch=320, lrate=0.300, error=10.224\n",
      ">epoch=340, lrate=0.300, error=12.315\n",
      ">epoch=360, lrate=0.300, error=10.473\n",
      ">epoch=380, lrate=0.300, error=10.764\n",
      ">epoch=400, lrate=0.300, error=4.501\n",
      ">epoch=420, lrate=0.300, error=10.527\n",
      ">epoch=440, lrate=0.300, error=7.873\n",
      ">epoch=460, lrate=0.300, error=7.426\n",
      ">epoch=480, lrate=0.300, error=9.325\n",
      ">epoch=0, lrate=0.300, error=104.711\n",
      ">epoch=20, lrate=0.300, error=85.042\n",
      ">epoch=40, lrate=0.300, error=85.029\n",
      ">epoch=60, lrate=0.300, error=47.240\n",
      ">epoch=80, lrate=0.300, error=43.958\n",
      ">epoch=100, lrate=0.300, error=42.244\n",
      ">epoch=120, lrate=0.300, error=41.348\n",
      ">epoch=140, lrate=0.300, error=41.025\n",
      ">epoch=160, lrate=0.300, error=40.514\n",
      ">epoch=180, lrate=0.300, error=40.192\n",
      ">epoch=200, lrate=0.300, error=40.204\n",
      ">epoch=220, lrate=0.300, error=39.715\n",
      ">epoch=240, lrate=0.300, error=40.137\n",
      ">epoch=260, lrate=0.300, error=40.237\n",
      ">epoch=280, lrate=0.300, error=39.700\n",
      ">epoch=300, lrate=0.300, error=39.150\n",
      ">epoch=320, lrate=0.300, error=38.044\n",
      ">epoch=340, lrate=0.300, error=38.682\n",
      ">epoch=360, lrate=0.300, error=39.772\n",
      ">epoch=380, lrate=0.300, error=37.755\n",
      ">epoch=400, lrate=0.300, error=38.713\n",
      ">epoch=420, lrate=0.300, error=36.888\n",
      ">epoch=440, lrate=0.300, error=36.464\n",
      ">epoch=460, lrate=0.300, error=36.528\n",
      ">epoch=480, lrate=0.300, error=38.547\n",
      "Scores: [96.66666666666667, 100.0, 100.0, 100.0, 66.66666666666666]\n",
      "Mean Accuracy: 92.667%\n"
     ]
    }
   ],
   "source": [
    "def initialize_network(input_number, hidden_number, output_number):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights': [random() for i in range(input_number + 1)]} for i in range(hidden_number)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights': [random() for i in range(hidden_number + 1)]} for i in range(output_number)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "\n",
    "def test(dataset, n_folds, input_number, output_number):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = bp(train_set, test_set, 0.3, 500, 5)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = measure_accuracy(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "\n",
    "def bp(train, test, l_rate, n_epoch, n_hidden):\n",
    "    input_number = len(train[0]) - 1\n",
    "    output_number = len(set([row[-1] for row in train]))\n",
    "    network = initialize_network(input_number, n_hidden, output_number)\n",
    "    train_network(network, train, l_rate, n_epoch, output_number)\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        outputs = feed_forward(network, row)\n",
    "        prediction = predict(network, row)\n",
    "        predictions.append(prediction)\n",
    "    return (predictions)\n",
    "\n",
    "def predict(network, row):\n",
    "    outputs = feed_forward(network, row)\n",
    "    return outputs.index(max(outputs))\n",
    "\n",
    "def measure_accuracy(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100\n",
    "\n",
    "seed(1)\n",
    "scores = test(irises, 5, 4, 3)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores) / float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
